{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f5e16f-40da-401b-90a9-9aaae9a17dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ce068c-66c1-4154-accb-e05b254770f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import scipy.stats\n",
    "import sklearn.neighbors\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1ccd8b-958e-4377-85f2-f3de06925cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = 'dhs_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4354f54f-451f-4981-bcc2-b5356bd20c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "535e67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test_masked.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fe92ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHSID_EA</th>\n",
       "      <th>cname</th>\n",
       "      <th>year</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>n_asset</th>\n",
       "      <th>asset_index</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>adm1fips</th>\n",
       "      <th>adm1dhs</th>\n",
       "      <th>urban</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IA-2015-7-00010009</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>9.220903</td>\n",
       "      <td>92.781530</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.721812</td>\n",
       "      <td>10009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_valid/IA-2015-7-00110896.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IA-2015-7-00010011</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>7.028410</td>\n",
       "      <td>93.883430</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.287279</td>\n",
       "      <td>10011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_valid/IA-2015-7-00120669.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IA-2015-7-00010017</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>12.371448</td>\n",
       "      <td>92.783665</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.677109</td>\n",
       "      <td>10017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_valid/IA-2015-7-00310941.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IA-2015-7-00010044</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>11.727304</td>\n",
       "      <td>92.719257</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.793683</td>\n",
       "      <td>10044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_valid/IA-2015-7-00332819.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IA-2015-7-00010060</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>9.185310</td>\n",
       "      <td>92.777645</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.758168</td>\n",
       "      <td>10060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_valid/IA-2015-7-00030310.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>IA-2015-7-00360403</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>17.410860</td>\n",
       "      <td>78.558810</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.823787</td>\n",
       "      <td>360403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>U</td>\n",
       "      <td>dhs_valid/IA-2015-7-00100002.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>IA-2015-7-00360454</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>17.450098</td>\n",
       "      <td>78.360138</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.990244</td>\n",
       "      <td>360454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_valid/IA-2015-7-00040683.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>IA-2015-7-00360474</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>17.973616</td>\n",
       "      <td>79.597408</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.633020</td>\n",
       "      <td>360474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>U</td>\n",
       "      <td>dhs_valid/IA-2015-7-00200056.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>IA-2015-7-00360476</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>18.956406</td>\n",
       "      <td>77.934361</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.429279</td>\n",
       "      <td>360476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_valid/IA-2015-7-00030367.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>IA-2015-7-00360479</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>17.571664</td>\n",
       "      <td>79.952492</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.972810</td>\n",
       "      <td>360479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_valid/IA-2015-7-00051115.npz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2675 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                DHSID_EA cname  year        lat        lon  n_asset  \\\n",
       "0     IA-2015-7-00010009    IA  2015   9.220903  92.781530     22.0   \n",
       "1     IA-2015-7-00010011    IA  2015   7.028410  93.883430     20.0   \n",
       "2     IA-2015-7-00010017    IA  2015  12.371448  92.783665     22.0   \n",
       "3     IA-2015-7-00010044    IA  2015  11.727304  92.719257     21.0   \n",
       "4     IA-2015-7-00010060    IA  2015   9.185310  92.777645     22.0   \n",
       "...                  ...   ...   ...        ...        ...      ...   \n",
       "2670  IA-2015-7-00360403    IA  2015  17.410860  78.558810     19.0   \n",
       "2671  IA-2015-7-00360454    IA  2015  17.450098  78.360138     22.0   \n",
       "2672  IA-2015-7-00360474    IA  2015  17.973616  79.597408     20.0   \n",
       "2673  IA-2015-7-00360476    IA  2015  18.956406  77.934361     21.0   \n",
       "2674  IA-2015-7-00360479    IA  2015  17.571664  79.952492     22.0   \n",
       "\n",
       "      asset_index  cluster_id  adm1fips  adm1dhs urban  \\\n",
       "0        2.721812       10009       NaN        1     R   \n",
       "1        2.287279       10011       NaN        1     R   \n",
       "2        0.677109       10017       NaN        1     R   \n",
       "3        1.793683       10044       NaN        1     R   \n",
       "4        2.758168       10060       NaN        1     R   \n",
       "...           ...         ...       ...      ...   ...   \n",
       "2670     2.823787      360403       NaN       36     U   \n",
       "2671     1.990244      360454       NaN       36     R   \n",
       "2672     2.633020      360474       NaN       36     U   \n",
       "2673     0.429279      360476       NaN       36     R   \n",
       "2674     0.972810      360479       NaN       36     R   \n",
       "\n",
       "                                  path  \n",
       "0     dhs_valid/IA-2015-7-00110896.npz  \n",
       "1     dhs_valid/IA-2015-7-00120669.npz  \n",
       "2     dhs_valid/IA-2015-7-00310941.npz  \n",
       "3     dhs_valid/IA-2015-7-00332819.npz  \n",
       "4     dhs_valid/IA-2015-7-00030310.npz  \n",
       "...                                ...  \n",
       "2670  dhs_valid/IA-2015-7-00100002.npz  \n",
       "2671  dhs_valid/IA-2015-7-00040683.npz  \n",
       "2672  dhs_valid/IA-2015-7-00200056.npz  \n",
       "2673  dhs_valid/IA-2015-7-00030367.npz  \n",
       "2674  dhs_valid/IA-2015-7-00051115.npz  \n",
       "\n",
       "[2675 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61d1443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHSID_EA</th>\n",
       "      <th>cname</th>\n",
       "      <th>year</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>n_asset</th>\n",
       "      <th>asset_index</th>\n",
       "      <th>n_water</th>\n",
       "      <th>water_index</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>adm1fips</th>\n",
       "      <th>adm1dhs</th>\n",
       "      <th>urban</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IA-2015-7-00010004</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>9.165413</td>\n",
       "      <td>92.742696</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.650768</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00192065.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IA-2015-7-00010005</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>8.307356</td>\n",
       "      <td>93.093792</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.157784</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00330893.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IA-2015-7-00010007</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>7.016968</td>\n",
       "      <td>93.893226</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.832751</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.619048</td>\n",
       "      <td>10007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00140183.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IA-2015-7-00010016</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>9.194938</td>\n",
       "      <td>92.800432</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.746096</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00261454.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IA-2015-7-00010018</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>8.055606</td>\n",
       "      <td>93.543892</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.581869</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00191303.npz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DHSID_EA cname  year       lat        lon  n_asset  asset_index  \\\n",
       "0  IA-2015-7-00010004    IA  2015  9.165413  92.742696     22.0     2.650768   \n",
       "1  IA-2015-7-00010005    IA  2015  8.307356  93.093792     22.0     2.157784   \n",
       "2  IA-2015-7-00010007    IA  2015  7.016968  93.893226     21.0     1.832751   \n",
       "3  IA-2015-7-00010016    IA  2015  9.194938  92.800432     22.0     2.746096   \n",
       "4  IA-2015-7-00010018    IA  2015  8.055606  93.543892     22.0     2.581869   \n",
       "\n",
       "   n_water  water_index  cluster_id  adm1fips  adm1dhs urban  \\\n",
       "0     22.0     5.000000       10004       NaN        1     R   \n",
       "1     22.0     5.000000       10005       NaN        1     R   \n",
       "2     21.0     4.619048       10007       NaN        1     R   \n",
       "3     22.0     5.000000       10016       NaN        1     R   \n",
       "4     22.0     5.000000       10018       NaN        1     R   \n",
       "\n",
       "                               path  \n",
       "0  dhs_train/IA-2015-7-00192065.npz  \n",
       "1  dhs_train/IA-2015-7-00330893.npz  \n",
       "2  dhs_train/IA-2015-7-00140183.npz  \n",
       "3  dhs_train/IA-2015-7-00261454.npz  \n",
       "4  dhs_train/IA-2015-7-00191303.npz  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b60b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"lat\", \"lon\"]\n",
    "label_cols = ['water_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051480a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "245389a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"lat\", \"lon\"]\n",
    "label_cols = ['water_index']\n",
    "train_X = train_df[feature_cols]\n",
    "train_Y = train_df[label_cols]\n",
    "test_X = test_df[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c50b383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (1.7.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from xgboost) (1.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from xgboost) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(train_X, train_Y)\n",
    "\n",
    "# Use the trained model to predict the labels of the test data\n",
    "test_Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c960c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y_pred\n",
    "\n",
    "solution_df = test_df.copy()\n",
    "\n",
    "solution_df[\"water_index\"] = test_Y_pred\n",
    "\n",
    "filtered_solution_df = solution_df[[\"DHSID_EA\", \"water_index\"]]\n",
    "filtered_solution_df.to_csv(\"solution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7941c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.165413</td>\n",
       "      <td>92.742696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.307356</td>\n",
       "      <td>93.093792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.016968</td>\n",
       "      <td>93.893226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.194938</td>\n",
       "      <td>92.800432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.055606</td>\n",
       "      <td>93.543892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18717</th>\n",
       "      <td>17.184983</td>\n",
       "      <td>79.989186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>17.465781</td>\n",
       "      <td>78.418820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>17.939231</td>\n",
       "      <td>79.512509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>17.420175</td>\n",
       "      <td>78.491013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18721</th>\n",
       "      <td>17.313331</td>\n",
       "      <td>79.963638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             lat        lon\n",
       "0       9.165413  92.742696\n",
       "1       8.307356  93.093792\n",
       "2       7.016968  93.893226\n",
       "3       9.194938  92.800432\n",
       "4       8.055606  93.543892\n",
       "...          ...        ...\n",
       "18717  17.184983  79.989186\n",
       "18718  17.465781  78.418820\n",
       "18719  17.939231  79.512509\n",
       "18720  17.420175  78.491013\n",
       "18721  17.313331  79.963638\n",
       "\n",
       "[18722 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a53e8638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.11.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.11.0) (2.11.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (3.7.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (23.3.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (0.30.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (15.0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (4.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (65.6.3)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (2.11.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (23.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (1.51.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (3.19.6)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow==2.11.0) (1.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow==2.11.0) (0.38.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (2.2.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (0.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\barya\\.conda\\envs\\posenet\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\barya\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow==2.11.0) (3.2.2)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "Successfully installed keras-2.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "intel-tensorflow 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 23.3.3 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.11.0 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.11.2 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.11.0 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.11.0 requires keras<2.12,>=2.11.0, but you have keras 2.4.3 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 23.3.3 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.4.3 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.11.2 which is incompatible.\n",
      "intel-tensorflow 2.9.1 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.11.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow==2.11.0\n",
    "!pip3 install -q keras==2.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc4358d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fd59ea6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2216\\3574964360.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# !{sys.executable} -m pip install xgboost\n",
    "\n",
    "import PIL \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.applications import VGG16\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# from keras.applications import VGG16\n",
    "# from keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test_masked.csv')\n",
    "\n",
    "# Define the pre-trained CNN model\n",
    "model = VGG16(weights=None, include_top=False, input_shape=(255,255,8))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from PIL import Image\n",
    "\n",
    "plt2 = plt\n",
    "# Define a function to extract features from an image\n",
    "def extract_features(filepath):\n",
    "    \n",
    "    # print(filepath)\n",
    "    # # Load the image data from the npz file\n",
    "    # np.savez_compressed(filepath, 'arr_0')\n",
    "    # # Load the image data from the npz file\n",
    "    # image_data = np.load(filepath)\n",
    "    \n",
    "    # print(image_data)\n",
    "    try:\n",
    "        with np.load(filepath) as data:\n",
    "            for key in data.keys():\n",
    "                # print(key)                        \n",
    "                # print(data[key].shape)  \n",
    "                # print(data[key]) \n",
    "                # im = Image.fromarray(data[key])\n",
    "                # im.save('./processed/' + key + '.png')\n",
    "                image_data = data[key]\n",
    "                image_data = np.moveaxis(image_data, 0, -1)\n",
    "                image_data.shape\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        image_data = np.zeros((255, 255, 8))\n",
    "    # Reshape the data to match the input shape of the model\n",
    "    image_data = image_data.reshape((1, image_data.shape[0], image_data.shape[1], image_data.shape[2]))\n",
    "    # Preprocess the image data for the model\n",
    "    image_data = preprocess_input(image_data)\n",
    "    # Use the model to extract features from the image\n",
    "    features = model.predict(image_data)\n",
    "    # Flatten the features to a 1-dimensional array\n",
    "    features = features.reshape((features.shape[0], -1))\n",
    "    return features\n",
    "\n",
    "# Extract features from all images in the training data\n",
    "train_image_features = []\n",
    "for filepath in train_df['path']:\n",
    "    features = extract_features(filepath)\n",
    "    train_image_features.append(features)\n",
    "train_image_features = np.array(train_image_features)\n",
    "\n",
    "# Extract features from all images in the test data\n",
    "test_image_features = []\n",
    "for filepath in test_df['path']:\n",
    "    features = extract_features(filepath)\n",
    "    test_image_features.append(features)\n",
    "test_image_features = np.array(test_image_features)\n",
    "\n",
    "# Combine the image features with the non-image features in the training data\n",
    "train_X = np.concatenate([train_df[feature_cols].values, train_image_features], axis=1)\n",
    "train_Y = train_df[label_cols]\n",
    "\n",
    "# Combine the image features with the non-image features in the test data\n",
    "test_X = np.concatenate([test_df[feature_cols].values, test_image_features], axis=1)\n",
    "\n",
    "# Train a model on the combined feature vectors\n",
    "model.fit(train_X, train_Y)\n",
    "\n",
    "# Use the trained model to predict the labels of the test data\n",
    "test_Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad416610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "(8, 255, 255)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2216\\625146351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;31m# print(data[key])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'RGB'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "with np.load('dhs_train/IA-2015-7-00360465.npz') as data:\n",
    "        for key in data.keys():\n",
    "            print(key)                        \n",
    "            print(data[key].shape)  \n",
    "            # print(data[key]) \n",
    "            im = Image.fromarray(data[key][3,:,:])\n",
    "            plt.imshow(data[key][3,:,:])\n",
    "            if im.mode != 'RGB':\n",
    "                im = im.convert('RGB')\n",
    "            im.save('./processed/' + 'aryan' + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59326a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHSID_EA</th>\n",
       "      <th>cname</th>\n",
       "      <th>year</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>n_asset</th>\n",
       "      <th>asset_index</th>\n",
       "      <th>n_water</th>\n",
       "      <th>water_index</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>adm1fips</th>\n",
       "      <th>adm1dhs</th>\n",
       "      <th>urban</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IA-2015-7-00010004</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>9.165413</td>\n",
       "      <td>92.742696</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.650768</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00192065.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IA-2015-7-00010005</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>8.307356</td>\n",
       "      <td>93.093792</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.157784</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00330893.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IA-2015-7-00010007</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>7.016968</td>\n",
       "      <td>93.893226</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.832751</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.619048</td>\n",
       "      <td>10007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00140183.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IA-2015-7-00010016</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>9.194938</td>\n",
       "      <td>92.800432</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.746096</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00261454.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IA-2015-7-00010018</td>\n",
       "      <td>IA</td>\n",
       "      <td>2015</td>\n",
       "      <td>8.055606</td>\n",
       "      <td>93.543892</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.581869</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>dhs_train/IA-2015-7-00191303.npz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DHSID_EA cname  year       lat        lon  n_asset  asset_index  \\\n",
       "0  IA-2015-7-00010004    IA  2015  9.165413  92.742696     22.0     2.650768   \n",
       "1  IA-2015-7-00010005    IA  2015  8.307356  93.093792     22.0     2.157784   \n",
       "2  IA-2015-7-00010007    IA  2015  7.016968  93.893226     21.0     1.832751   \n",
       "3  IA-2015-7-00010016    IA  2015  9.194938  92.800432     22.0     2.746096   \n",
       "4  IA-2015-7-00010018    IA  2015  8.055606  93.543892     22.0     2.581869   \n",
       "\n",
       "   n_water  water_index  cluster_id  adm1fips  adm1dhs urban  \\\n",
       "0     22.0     5.000000       10004       NaN        1     R   \n",
       "1     22.0     5.000000       10005       NaN        1     R   \n",
       "2     21.0     4.619048       10007       NaN        1     R   \n",
       "3     22.0     5.000000       10016       NaN        1     R   \n",
       "4     22.0     5.000000       10018       NaN        1     R   \n",
       "\n",
       "                               path  \n",
       "0  dhs_train/IA-2015-7-00192065.npz  \n",
       "1  dhs_train/IA-2015-7-00330893.npz  \n",
       "2  dhs_train/IA-2015-7-00140183.npz  \n",
       "3  dhs_train/IA-2015-7-00261454.npz  \n",
       "4  dhs_train/IA-2015-7-00191303.npz  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f82887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barya\\AppData\\Local\\Temp\\ipykernel_32744\\2464559816.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X['urban'] = train_X['urban'].map({'R': 1, 'U': 0})\n",
      "C:\\Users\\barya\\AppData\\Local\\Temp\\ipykernel_32744\\2464559816.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_X['urban'] = test_X['urban'].map({'R': 1, 'U': 0})\n",
      "C:\\Users\\barya\\AppData\\Local\\Temp\\ipykernel_32744\\2464559816.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(train_X, train_Y)\n",
      "c:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\barya\\AppData\\Local\\Temp\\ipykernel_32744\\2464559816.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(train_X, train_Y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "feature_cols = [\"lat\", \"lon\", \"n_asset\" , \"asset_index\", \"urban\", 'cluster_id', 'adm1dhs']\n",
    "label_cols = ['water_index']\n",
    "train_X = train_df[feature_cols]\n",
    "train_Y = train_df[label_cols]\n",
    "test_X = test_df[feature_cols]\n",
    "\n",
    "# change all entries in the urban column from R to 1 and U to 0\n",
    "train_X['urban'] = train_X['urban'].map({'R': 1, 'U': 0})\n",
    "test_X['urban'] = test_X['urban'].map({'R': 1, 'U': 0})\n",
    "\n",
    "\n",
    "# Define the base models to use in the ensemble\n",
    "base_models = [\n",
    "    # LinearRegression(),\n",
    "    # BaggingRegressor(n_estimators=50, random_state=42),\n",
    "    RandomForestRegressor(n_estimators=50, random_state=42),\n",
    "    GradientBoostingRegressor(n_estimators=50, random_state=42),\n",
    "    AdaBoostRegressor(n_estimators=50, random_state=42),\n",
    "    ExtraTreesRegressor(n_estimators=50, random_state=42),\n",
    "    # VotingRegressor(estimators=[('rf', RandomForestRegressor(n_estimators=50, random_state=42)), ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42)), ('ab', AdaBoostRegressor(n_estimators=50, random_state=42)), ('et', ExtraTreesRegressor(n_estimators=50, random_state=42))])\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "# Train each base model on the training data and make predictions on the testing data\n",
    "base_model_predictions = []\n",
    "for model in base_models:\n",
    "    model.fit(train_X, train_Y)\n",
    "    base_model_predictions.append(model.predict(test_X))\n",
    "    \n",
    "\n",
    "\n",
    "# Combine the predictions of the base models by taking their average\n",
    "# ensemble_predictions = sum(base_model_predictions) / len(base_model_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7686744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>n_asset</th>\n",
       "      <th>asset_index</th>\n",
       "      <th>urban</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>adm1dhs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.165413</td>\n",
       "      <td>92.742696</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.650768</td>\n",
       "      <td>1</td>\n",
       "      <td>10004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.307356</td>\n",
       "      <td>93.093792</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.157784</td>\n",
       "      <td>1</td>\n",
       "      <td>10005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.016968</td>\n",
       "      <td>93.893226</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.832751</td>\n",
       "      <td>1</td>\n",
       "      <td>10007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.194938</td>\n",
       "      <td>92.800432</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.746096</td>\n",
       "      <td>1</td>\n",
       "      <td>10016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.055606</td>\n",
       "      <td>93.543892</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.581869</td>\n",
       "      <td>1</td>\n",
       "      <td>10018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18717</th>\n",
       "      <td>17.184983</td>\n",
       "      <td>79.989186</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.629311</td>\n",
       "      <td>1</td>\n",
       "      <td>360473</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>17.465781</td>\n",
       "      <td>78.418820</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.704372</td>\n",
       "      <td>0</td>\n",
       "      <td>360475</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>17.939231</td>\n",
       "      <td>79.512509</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.315361</td>\n",
       "      <td>1</td>\n",
       "      <td>360480</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>17.420175</td>\n",
       "      <td>78.491013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.920893</td>\n",
       "      <td>0</td>\n",
       "      <td>360481</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18721</th>\n",
       "      <td>17.313331</td>\n",
       "      <td>79.963638</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.985008</td>\n",
       "      <td>1</td>\n",
       "      <td>360482</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18722 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             lat        lon  n_asset  asset_index  urban  cluster_id  adm1dhs\n",
       "0       9.165413  92.742696     22.0     2.650768      1       10004        1\n",
       "1       8.307356  93.093792     22.0     2.157784      1       10005        1\n",
       "2       7.016968  93.893226     21.0     1.832751      1       10007        1\n",
       "3       9.194938  92.800432     22.0     2.746096      1       10016        1\n",
       "4       8.055606  93.543892     22.0     2.581869      1       10018        1\n",
       "...          ...        ...      ...          ...    ...         ...      ...\n",
       "18717  17.184983  79.989186     22.0     0.629311      1      360473       36\n",
       "18718  17.465781  78.418820     20.0     2.704372      0      360475       36\n",
       "18719  17.939231  79.512509     17.0     1.315361      1      360480       36\n",
       "18720  17.420175  78.491013     23.0     2.920893      0      360481       36\n",
       "18721  17.313331  79.963638     21.0     0.985008      1      360482       36\n",
       "\n",
       "[18722 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb4b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76701376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean of all the predictions in one array \n",
    "ensemble_predictions = np.zeros(len(base_model_predictions[0]))\n",
    "for i in range(len(base_model_predictions)):\n",
    "    if i == 0:\n",
    "        ensemble_predictions = base_model_predictions[i]\n",
    "    else:\n",
    "        ensemble_predictions += base_model_predictions[i]\n",
    "\n",
    "ensemble_predictions = ensemble_predictions / len(base_model_predictions)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0079da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([19.77668811, 19.061449  , 17.55640745, ..., 19.31940015,\n",
       "        17.290865  , 17.08592288]),\n",
       " array([4.95061687, 4.73452243, 4.50296768, ..., 4.9019302 , 4.27769927,\n",
       "        4.53488461]),\n",
       " array([4.82788942, 4.50118547, 4.24971683, ..., 4.70892402, 4.03881356,\n",
       "        4.24971683]),\n",
       " array([5.        , 4.92996864, 4.38597403, ..., 4.84293106, 4.42484279,\n",
       "        4.11468267])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9541ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.94417203, 4.76536225, 4.38910186, ..., 4.82985004, 4.32271625,\n",
       "       4.27148072])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4973ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_df = test_df.copy()\n",
    "\n",
    "solution_df[\"water_index\"] = ensemble_predictions\n",
    "\n",
    "filtered_solution_df = solution_df[[\"DHSID_EA\", \"water_index\"]]\n",
    "filtered_solution_df.to_csv(\"solution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24ab7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'transpose_shape' from 'keras.utils.generic_utils' (c:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\utils\\generic_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv2D, MaxPooling2D, Flatten, Dense\n",
      "File \u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n",
      "File \u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\distribute\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras' Distribution Strategy library.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m sidecar_evaluator\n",
      "File \u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_logging \u001b[39mas\u001b[39;00m logging\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecation\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer_experimental\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     optimizer \u001b[39mas\u001b[39;00m optimizer_experimental,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m keras_export\n\u001b[0;32m     27\u001b[0m _PRINT_EVAL_STEP_EVERY_SEC \u001b[39m=\u001b[39m \u001b[39m60.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\optimizers\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# Imports needed for deserialization.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m adadelta \u001b[39mas\u001b[39;00m adadelta_legacy\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m adagrad \u001b[39mas\u001b[39;00m adagrad_legacy\n",
      "File \u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\backend\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mload_backend\u001b[39;00m \u001b[39mimport\u001b[39;00m epsilon\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mload_backend\u001b[39;00m \u001b[39mimport\u001b[39;00m set_epsilon\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mload_backend\u001b[39;00m \u001b[39mimport\u001b[39;00m floatx\n",
      "File \u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\backend\\load_backend.py:90\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39melif\u001b[39;00m _BACKEND \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     89\u001b[0m     sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39mUsing TensorFlow backend.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtensorflow_backend\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[39m# Try and load external backend.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdistutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m StrictVersion\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m transpose_shape\n\u001b[0;32m     27\u001b[0m py_all \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m\n\u001b[0;32m     28\u001b[0m py_any \u001b[39m=\u001b[39m \u001b[39many\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'transpose_shape' from 'keras.utils.generic_utils' (c:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\utils\\generic_utils.py)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the DeepSDG model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(8, 255, 255)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c35f7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "# select 500 random samples from the training dataset\n",
    "train_subset = train_dataset.sample(n=1000, random_state=42)\n",
    "\n",
    "# select 100 random samples from the training dataset for validation\n",
    "# test_dataset = train_dataset.sample(n=100, random_state=42)\n",
    "\n",
    "validation_dataset = train_dataset.sample(n=100, random_state=42)\n",
    "\n",
    "training_images = []\n",
    "for filepath in train_subset['path']:\n",
    "    try:\n",
    "        with np.load(filepath) as data:\n",
    "            for key in data.keys():\n",
    "                image_data = data[key]\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        image_data = np.zeros((255, 255, 8))\n",
    "    training_images.append(image_data)\n",
    "\n",
    "training_scores = train_subset['water_index'].values\n",
    "\n",
    "# testing_images = []\n",
    "# for filepath in test_dataset['path']:\n",
    "#     try:\n",
    "#         with np.load(filepath) as data:\n",
    "#             for key in data.keys():\n",
    "#                 image_data = data[key]\n",
    "#     except:\n",
    "#         print(\"Error\")\n",
    "#         image_data = np.zeros((255, 255, 8))\n",
    "#     testing_images.append(image_data)\n",
    "\n",
    "# testing_scores = test_dataset['water_index'].values\n",
    "\n",
    "\n",
    "validation_images = []\n",
    "for filepath in validation_dataset['path']:\n",
    "    try:\n",
    "        with np.load(filepath) as data:\n",
    "            for key in data.keys():\n",
    "                image_data = data[key]\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        image_data = np.zeros((255, 255, 8))\n",
    "    validation_images.append(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4f54c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/GPU:0\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 45\u001b[0m     model\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(training_images), np\u001b[39m.\u001b[39;49marray(training_scores), epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n\u001b[0;32m     47\u001b[0m \u001b[39m# Generate some dummy data for prediction\u001b[39;00m\n\u001b[0;32m     48\u001b[0m x_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(validation_images)\n",
      "File \u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\barya\\.conda\\envs\\posenet\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.backend import set_floatx\n",
    "\n",
    "set_floatx('float32')\n",
    "# tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (8, 255, 255)\n",
    "\n",
    "# Define the input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Define the convolutional layers\n",
    "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(pool2)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "# Flatten the output of the convolutional layers\n",
    "flatten = Flatten()(pool3)\n",
    "\n",
    "# Define the fully connected layers\n",
    "fc1 = Dense(512, activation='relu')(flatten)\n",
    "dropout1 = Dropout(0.5)(fc1)\n",
    "fc2 = Dense(256, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.5)(fc2)\n",
    "\n",
    "# Define the output layer\n",
    "output = Dense(1, activation='linear')(dropout2)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(np.array(training_images), np.array(training_scores), epochs=20, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# Generate some dummy data for prediction\n",
    "x_pred = np.array(validation_images)\n",
    "\n",
    "\n",
    "# Use the model to make predictions\n",
    "with tf.device('/GPU:0'):\n",
    "    y_pred = model.predict(x_pred)\n",
    "\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ce23a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11676952372607146469\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 1736599143\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 11824712242411943943\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8dd033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
